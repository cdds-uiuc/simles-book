{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eadd0852-b63c-4fef-85e0-187fdeb9a7d0",
   "metadata": {},
   "source": [
    "A large part of statistical inference deals with estimating properties of a random process from a given data. In the previous notebook we talked about the maximum likelihood estimator, as the estimator obtained by maximizing the likelihood that the data x was drawn from a process with parameters \\theta.\n",
    "\n",
    "$$\\hat{\\theta}_{mle}=\\arg\\max_{\\theta}\\left\\{ p\\left(x|\\theta\\right)\\right\\} $$\n",
    "\n",
    "There are other estimators we can use. We talked about how we can approximate (i.e. estimate!) the variance of a random process using n samples \\left\\{ x_{1}\\ldots x_{n}\\right\\}  as:\n",
    "\n",
    "s_{n}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}_{n}\\right)^{2}\n",
    "\n",
    "And we talked about how we could alter that estimator using Bessel's correction to yield a different estimator of the variance: s_{n-1}=\\frac{n}{n-1}s_{n}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}_{n}\\right)^{2}\n",
    "\n",
    "We talked about how this correctoin might be desirable (sometimes) because it yields an unbiased estimator. \n",
    "\n",
    "Method of moments. \n",
    "\n",
    "We can build estimators in other ways. For example, we can use the method of moments, where we simply estimate the distribution by matching the process moments with the sample moments. This might be more evident with a few examples:\n",
    "\n",
    "Bernoulli Distribution\n",
    "\n",
    "Let's say we have a sample of size N from a coin with probablity p of hitting heads. \n",
    "\n",
    "\\mathbf{x}=\\left\\{ x_{1},x_{2},\\ldots,x_{n}\\right\\} \n",
    "\n",
    "drawn from X\\sim Bern\\left(p\\right)\n",
    "\n",
    "And we want to estimate the value of the parameter p. The first moment of the process (or population) is\n",
    "\n",
    "\\mu=E\\left(X\\right)=p\n",
    "\n",
    "And the sample mean is:\\overline{x}_{n}=\\frac{1}{n}x_{i}=\\frac{n_{1}}{n_{0}+n_{1}} where n_{0} and n_{1} are the number of tails (zeros) and heads (ones). \n",
    "\n",
    "The method of moments comes from setting \\mu=\\overline{x}_{n}, so\n",
    "\n",
    "\\hat{p}_{mm}=\\frac{n_{1}}{n_{0}+n_{1}}\n",
    "\n",
    "Notice that in this case, the methods of moment estimator \\hat{p}_{mm} is the same as the maximum likelihood estimator \\hat{p}_{mle}.\n",
    "\n",
    "The same holds for a Gaussian distribution, where \\hat{\\mu}_{mle}=\\overline{x}_{n} and \\hat{\\sigma}_{mle}=s_{n}. But that is not universally true:\n",
    "\n",
    "Gamma Distribution:\n",
    "\n",
    "Method of moments\n",
    "\n",
    "The process/population moments of X\\sim\\Gamma\\left(\\alpha,\\beta\\right) are\n",
    "\n",
    "\\begin{cases}\n",
    "m_{1}=E(X)=\\mu=\\frac{\\alpha}{\\beta}\\\\\n",
    "m_{2}=E\\left(X^{2}\\right)=E\\left(X^{2}\\right)-E(X)^{2}+E(X)^{2}=\\sigma^{2}+\\mu^{2}=\\frac{\\alpha}{\\beta^{2}}+\\frac{\\alpha^{2}}{\\beta^{2}}\n",
    "\\end{cases}\n",
    "\n",
    "By matching these with sample moments we get the equation for the method of moments estimator:\n",
    "\n",
    "\\begin{cases}\n",
    "\\hat{\\mu}=\\overline{x}_{n}=\\frac{\\hat{\\alpha}_{mm}}{\\hat{\\beta}_{mm}}\\\\\n",
    "\\hat{\\sigma}^{2}+\\hat{\\mu}^{2}=\\overline{x}_{n}^{2}+s_{n}=\\frac{\\hat{\\alpha}_{mm}^{2}}{\\hat{\\beta}_{mm}^{2}}+\\frac{\\hat{\\alpha}_{mm}}{\\hat{\\beta}_{mm}^{2}}\n",
    "\\end{cases}\n",
    "\n",
    "which yields\n",
    "\n",
    "\\begin{cases}\n",
    "\\hat{\\alpha}_{mm}=\\frac{\\overline{x}_{n}^{2}}{s_{n}}\\\\\n",
    "\\hat{\\beta}_{mm}=\\frac{\\overline{x}_{n}}{s_{n}}\n",
    "\\end{cases}\n",
    "\n",
    "By contrast, the maximum likelihood estimator is obtained from maximizing the likelihood:\n",
    "\n",
    "\\log\\mathcal{L}\\left(\\alpha,\\beta\\right)=\\sum_{i=1}^{n}\\left(-\\log\\Gamma(\\alpha)-\\alpha\\log\\beta+\\left(\\alpha-1\\right)\\log\\left(x_{i}-x_{o}\\right)-\\frac{\\left(x_{i}-x_{o}\\right)}{\\beta}\\right)\n",
    "\n",
    "and the mle estimators for \\alpha and \\beta are obtained by setting the partial derivatives to zero. Thus the mle estimators satsifyL\n",
    "\n",
    "\\begin{cases}\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\alpha}\\left(\\hat{\\alpha}{}_{mle},\\hat{\\beta}{}_{mle}\\right)=0\\\\\n",
    "\\frac{\\partial L}{\\partial\\beta}\\left(\\hat{\\alpha}{}_{mle},\\hat{\\beta}{}_{mle}\\right)=0\n",
    "\\end{cases}\n",
    "\n",
    "\\begin{cases}\n",
    "n\\left(\\ln\\hat{\\beta}{}_{mle}-\\frac{d}{d\\alpha}\\ln\\Gamma\\left(\\hat{\\alpha}{}_{mle}\\right)\\right)+\\sum_{i=1}^{n}\\ln\\left(x_{i}\\right)=0\\\\\n",
    "n\\frac{\\hat{\\alpha}{}_{mle}}{\\hat{\\beta}{}_{mle}}-\\sum_{i=1}^{n}x_{i}=0\n",
    "\\end{cases}\n",
    "\n",
    "Which can be rewritten as:\n",
    "\n",
    "\\begin{cases}\n",
    "n\\left(\\ln\\hat{\\alpha}-\\ln\\overline{x}_{n}-\\frac{d}{d\\alpha}\\ln\\Gamma\\left(\\alpha\\right)+\\overline{\\ln\\left(x_{i}\\right)}\\right)=0\\\\\n",
    "\\hat{\\beta}{}_{mle}=\\frac{\\hat{\\alpha}{}_{mle}}{\\overline{x}_{n}}\n",
    "\\end{cases}\n",
    "\n",
    "Notice that for the gamma distribution, the mle and method of moments estimators are not identical. Indeed the stats.gamma.fit function in scipy allows one to fit alpha and beta using two different methods. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dbd94a-6863-4a88-b94a-e8a23e3668ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

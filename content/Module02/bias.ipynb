{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eadd0852-b63c-4fef-85e0-187fdeb9a7d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "A large part of statistical inference deals with estimating properties of a random process from a given data. In the previous notebook we talked about the maximum likelihood estimator, as the estimator obtained by maximizing the likelihood that the data $x$ was drawn from a process with parameters $\\theta$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{mle}=\\arg\\max_{\\theta}\\left\\{ p\\left(x|\\theta\\right)\\right\\} \n",
    "$$\n",
    "\n",
    "There are other estimators we can use. We talked about how we can\n",
    "approximate (i.e. estimate!) the variance of a random process using\n",
    "$n$ samples $\\left\\{ x_{1}\\ldots x_{n}\\right\\} $ as:\n",
    "\n",
    "$$\n",
    "s_{n}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}_{n}\\right)^{2}\n",
    "$$\n",
    "\n",
    "And we talked about how we could alter that estimator using Bessel's\n",
    "correction to yield a different estimator of the variance: \n",
    "$$\n",
    "s_{n-1}=\\frac{n}{n-1}s_{n}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}_{n}\\right)^{2}\n",
    "$$\n",
    "\n",
    "We talked about how this correctoin might be desirable (sometimes)\n",
    "because it yields an unbiased estimator. \n",
    "\n",
    "# Method of moments \n",
    "\n",
    "\n",
    "We can build estimators in other ways. For example, we can use the\n",
    "method of moments, where we simply estimate the distribution by matching\n",
    "the process moments with the sample moments. This might be more evident\n",
    "with a few examples:\n",
    "\n",
    "## Bernoulli Distribution\n",
    "\n",
    "Let's say we have a sample of size $n$, $\\mathbf{x}=\\left\\{ x_{1},x_{2},\\ldots,x_{n}\\right\\}$  drawm from a coin with probablity\n",
    "$p$ of hitting heads, $X\\sim Bern\\left(p\\right)$.  We want to estimate the value of the parameter $p$. The first\n",
    "moment of the process (or population) is\n",
    "\n",
    "$$ \\mu=E\\left(X\\right)=p $$\n",
    "\n",
    "And the sample mean is:\n",
    "\n",
    "$$ \\overline{x}_{n}=\\frac{1}{n}\\sum_{i=1}{n}x_{i}=\\frac{n_{1}}{n_{0}+n_{1}} $$\n",
    "\n",
    "where $n_{0}$ and $n_{1}$ are the number of tails (zeros) and heads\n",
    "(ones). The method of moments involves matching the process and sample moments. Setting $\\mu=\\overline{x}_{n}$, we get\n",
    "\n",
    "$$ \\hat{p}_{mm}=\\frac{n_{1}}{n_{0}+n_{1}} $$\n",
    "\n",
    "Notice that in this case, the methods of moment estimator $\\hat{p}_{mm}$\n",
    "is the same as the maximum likelihood estimator $\\hat{p}_{mle}$.\n",
    "\n",
    "The same holds for a Gaussian distribution, where $\\hat{\\mu}_{mle}=\\overline{x}_{n}$\n",
    "and $\\hat{\\sigma}_{mle}=s_{n}$. But that is not universally true:\n",
    "\n",
    "## Gamma Distribution\n",
    "\n",
    "### Method of moments\n",
    "\n",
    "The process/population moments of $X\\sim\\Gamma\\left(\\alpha,\\beta\\right)$\n",
    "are\n",
    "\n",
    "$$ \\begin{cases}\n",
    "m_{1}=E(X)=\\mu=\\frac{\\alpha}{\\beta}\\\\\n",
    "m_{2}=E\\left(X^{2}\\right)=E\\left(X^{2}\\right)-E(X)^{2}+E(X)^{2}=\\sigma^{2}+\\mu^{2}=\\frac{\\alpha}{\\beta^{2}}+\\frac{\\alpha^{2}}{\\beta^{2}}\n",
    "\\end{cases} $$\n",
    "\n",
    "By matching these with sample moments we get the equation for the\n",
    "method of moments estimator:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\hat{\\mu}=\\overline{x}_{n}=\\frac{\\hat{\\alpha}_{mm}}{\\hat{\\beta}_{mm}}\\\\\n",
    "\\hat{\\sigma}^{2}+\\hat{\\mu}^{2}=\\overline{x}_{n}^{2}+s_{n}=\\frac{\\hat{\\alpha}_{mm}^{2}}{\\hat{\\beta}_{mm}^{2}}+\\frac{\\hat{\\alpha}_{mm}}{\\hat{\\beta}_{mm}^{2}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "which yields\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\hat{\\alpha}_{mm}=\\frac{\\overline{x}_{n}^{2}}{s_{n}}\\\\\n",
    "\\hat{\\beta}_{mm}=\\frac{\\overline{x}_{n}}{s_{n}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "By contrast, the maximum likelihood estimator is obtained from maximizing\n",
    "the likelihood:\n",
    "\n",
    "$$\n",
    "\\log\\mathcal{L}\\left(\\alpha,\\beta\\right)=\\sum_{i=1}^{n}\\left(-\\log\\Gamma(\\alpha)-\\alpha\\log\\beta+\\left(\\alpha-1\\right)\\log\\left(x_{i}-x_{o}\\right)-\\frac{\\left(x_{i}-x_{o}\\right)}{\\beta}\\right)\n",
    "$$\n",
    "\n",
    "and the mle estimators for $\\alpha$ and $\\beta$ are obtained by\n",
    "setting the partial derivatives to zero. Thus the mle estimators satsify\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\alpha}\\left(\\hat{\\alpha}{}_{mle},\\hat{\\beta}{}_{mle}\\right)=0\\\\\n",
    "\\frac{\\partial L}{\\partial\\beta}\\left(\\hat{\\alpha}{}_{mle},\\hat{\\beta}{}_{mle}\\right)=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "n\\left(\\ln\\hat{\\beta}{}_{mle}-\\frac{d}{d\\alpha}\\ln\\Gamma\\left(\\hat{\\alpha}{}_{mle}\\right)\\right)+\\sum_{i=1}^{n}\\ln\\left(x_{i}\\right)=0\\\\\n",
    "n\\frac{\\hat{\\alpha}{}_{mle}}{\\hat{\\beta}{}_{mle}}-\\sum_{i=1}^{n}x_{i}=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Which can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "n\\left(\\ln\\hat{\\alpha}-\\ln\\overline{x}_{n}-\\frac{d}{d\\alpha}\\ln\\Gamma\\left(\\alpha\\right)+\\overline{\\ln\\left(x\\right)}_{n}\\right)=0\\\\\n",
    "\\hat{\\beta}{}_{mle}=\\frac{\\hat{\\alpha}{}_{mle}}{\\overline{x}_{n}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Notice that for the gamma distribution, the mle and method of moments\n",
    "estimators are not identical. Indeed the stats.gamma.fit function\n",
    "in scipy allows one to fit alpha and beta using two different methods. \n",
    "\n",
    "We can build yet other estimators. Later in the course we will spend\n",
    "a lot of time looking at a class of estimators called regularized\n",
    "estimators. But in principle, we could build any kind of estimator.\n",
    "For example, we could always estimate the sample-mean of the population\n",
    "to be zero. That would be a very poor estimator, but nonetheless it\n",
    "would be an estimator. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dbd94a-6863-4a88-b94a-e8a23e3668ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
